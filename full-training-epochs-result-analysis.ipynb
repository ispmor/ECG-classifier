{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training script\n",
      "cuda:1\n",
      "Selected device: GeForce GTX 980 Ti\n",
      "Considered classes: ['RBBB', 'I-AVB', 'STD', 'PAC', 'AF', 'PVC', 'Normal', 'LBBB', 'STE']\n"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "\n",
    "from nbeats_pytorch.model import NBeatsNet\n",
    "import nbeats_additional_functions as naf\n",
    "import os\n",
    "import torch\n",
    "from torch import optim\n",
    "from config import default_net_params as dnp\n",
    "\n",
    "print(\"Starting training script\")\n",
    "\n",
    "checkpoint_name_BASE = \"nbeats_checkpoint\"\n",
    "checkpoint_training_base = \"_training\"\n",
    "\n",
    "data_dir = os.path.dirname(os.getcwd()) + \"/data/\"\n",
    "models = os.path.dirname(os.getcwd()) + \"/models/\"\n",
    "training_models = os.path.dirname(os.getcwd()) + \"/models/training/\"\n",
    "\n",
    "if not os.path.exists(models):\n",
    "    os.mkdir(models)\n",
    "if not os.path.exists(training_models):\n",
    "    os.mkdir(training_models)\n",
    "\n",
    "\n",
    "\n",
    "d = [x[0] for x in os.walk(data_dir)]\n",
    "dirs = []\n",
    "for directory_name in d:\n",
    "    if ',' not in directory_name:\n",
    "        directory_name = directory_name.split(\"/\")[-1]\n",
    "        if all(x not in directory_name for x in [\",\", \"training\", \"test\"]):\n",
    "            dirs.append(directory_name)\n",
    "dirs = dirs[1:]\n",
    "\n",
    "threshold = 0.0001\n",
    "limit = 600\n",
    "plot_eval = False\n",
    "\n",
    "# Bart\n",
    "\n",
    "cuda1 = torch.cuda.set_device(1)\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "print(device)\n",
    "print(\"Selected device: %s\" % (torch.cuda.get_device_name(1)))\n",
    "torch.pin_memory=False\n",
    "\n",
    "print(\"Considered classes: %s\" % (dirs))\n",
    "\n",
    "#neptune.init('puszkarb/ecg-dyplom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_grad_steps(data, device, net, optimiser, test_losses, training_checkpoint, size):\n",
    "    global_step = load(training_checkpoint, net, optimiser)\n",
    "    local_step = 0\n",
    "    for x_train_batch, y_train_batch in data:\n",
    "        global_step += 1\n",
    "        local_step += 1\n",
    "        optimiser.zero_grad()\n",
    "        net.train()\n",
    "        _, forecast = net(x_train_batch.clone().detach())#.to(device))\n",
    "        loss = F.mse_loss(forecast, y_train_batch.clone().detach())#.to(device))\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        if global_step > 0 and global_step % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                save(training_checkpoint, net, optimiser, global_step)\n",
    "                return global_step\n",
    "        if local_step > 0 and local_step % size == 0:\n",
    "            print(local_step)\n",
    "            return global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = dnp.batch_size\n",
    "\n",
    "for folder_name in dirs:\n",
    "    experiment = neptune.create_experiment(name=folder_name + f'-f{forecast_length}-b{backcast_length}-btch{batch_size}-h{hidden}')\n",
    "   \n",
    "\n",
    "    name = folder_name.split(\"/\")[-1]\n",
    "    checkpoint_name = name + \"_\" + checkpoint_name_BASE+ f'-f{forecast_length}-b{backcast_length}-btch{batch_size}-h{hidden}'\n",
    "    training_checkpoint = name + checkpoint_training_base + f'-f{forecast_length}-b{backcast_length}-btch{batch_size}-h{hidden}' + \".th\"\n",
    "\n",
    "\n",
    "    if os.path.isfile(training_checkpoint):\n",
    "        continue\n",
    "\n",
    "    net = NBeatsNet(stack_types=[NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK],\n",
    "                    forecast_length= dnp.forecast_length,\n",
    "                    thetas_dims=[7, 8],\n",
    "                    nb_blocks_per_stack=3,\n",
    "                    backcast_length=dnp.backcast_length,\n",
    "                    hidden_layer_units=dnp.hidden_layer_units,\n",
    "                    share_weights_in_stack=False,\n",
    "                    device=device)\n",
    "    net.cuda()\n",
    "    optimiser = optim.Adam(net.parameters())\n",
    "\n",
    "    test_losses = []\n",
    "    old_eval = 100\n",
    "    the_lowest_error = [100]\n",
    "    old_checkpoint = \"\"\n",
    "    actual_class_dir = data_dir + name + \"/\"\n",
    "    print(\"N-Beats training for class: %s\" % (actual_class_dir))\n",
    "    iteration = 0\n",
    "\n",
    "    for (_, dirs, files) in os.walk(actual_class_dir):\n",
    "        difference = 1000\n",
    "        for file in files:\n",
    "            i = 0\n",
    "            if 'mat' in file:\n",
    "                continue\n",
    "\n",
    "            iteration += 1\n",
    "            if iteration > 100 or difference < threshold:\n",
    "                break\n",
    "\n",
    "            data, x_train, y_train, x_test, y_test, norm_constant = naf.one_file_training_data(actual_class_dir,\n",
    "                                                                                               file,\n",
    "                                                                                               forecast_length,\n",
    "                                                                                               backcast_length,\n",
    "                                                                                               batch_size,\n",
    "                                                                                               device)\n",
    "            \n",
    "\n",
    "            while i < limit:  #difference > threshold and\n",
    "                i += 1\n",
    "                \n",
    "                global_step = train_full_grad_steps(data,\n",
    "                                                        device,\n",
    "                                                        net,\n",
    "                                                        optimiser,\n",
    "                                                        test_losses, \n",
    "                                                        training_models+training_checkpoint,\n",
    "                                                        x_train.shape[0])\n",
    "                \n",
    "                train_eval = naf.evaluate_training(backcast_length,\n",
    "                                                   forecast_length,\n",
    "                                                   net,\n",
    "                                                   norm_constant,\n",
    "                                                   test_losses,\n",
    "                                                   x_train,\n",
    "                                                   y_train,\n",
    "                                                   the_lowest_error,\n",
    "                                                   device)\n",
    "                experiment.log_metric('train_loss', train_eval)\n",
    "                \n",
    "               \n",
    "                new_eval = naf.evaluate_training(backcast_length,\n",
    "                                                 forecast_length,\n",
    "                                                 net,\n",
    "                                                 norm_constant,\n",
    "                                                 test_losses,\n",
    "                                                 x_test,\n",
    "                                                 y_test, \n",
    "                                                 the_lowest_error,\n",
    "                                                 device,\n",
    "                                                 plot_eval=False,\n",
    "                                                 class_dir=name,\n",
    "                                                 step=i)\n",
    "                experiment.log_metric('eval_loss', new_eval)\n",
    "                \n",
    "                print(\"File: %s\\t Training Loop: %d/%d, New evaluation sccore: %f\" % (file, i, limit, new_eval), end=\"\\r\")\n",
    "                if new_eval < old_eval:\n",
    "                    difference = old_eval - new_eval\n",
    "                    old_eval = new_eval\n",
    "                    with torch.no_grad():\n",
    "                        if old_checkpoint:\n",
    "                            os.remove(models+old_checkpoint)\n",
    "                        new_checkpoint_name = str(checkpoint_name[:-3] + str(len(test_losses)) + \".th\")\n",
    "                        naf.save(models + new_checkpoint_name, net, optimiser, global_step)\n",
    "                        old_checkpoint = new_checkpoint_name\n",
    "                        \n",
    "    experiment.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
